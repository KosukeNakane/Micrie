# -----------------------------------------------
# ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã€
# KICK / SNARE / HIHAT / NOISE ã‚’åˆ†é¡ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ãƒ»ä¿å­˜
# ã¾ãŸã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸ8åˆ†å‰²äºˆæ¸¬é–¢æ•°ã‚‚å«ã‚€
# -----------------------------------------------
import os
import numpy as np
import tensorflow as tf
import librosa
from sklearn.model_selection import train_test_split

DATASET_DIR = "./dataset"
CATEGORIES = ["kick", "snare", "hihat", "noise"]

# ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªæ³¢å½¢ y ã‹ã‚‰å„ç¨®éŸ³éŸ¿ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹é–¢æ•°
def extract_features_from_y(y, sr):
    # ãƒãƒ£ãƒ³ã‚¯ã®æœ«å°¾10%ã‚’ã‚«ãƒƒãƒˆï¼ˆç™ºéŸ³ã®ã‚ã‚Šè¾¼ã¿å¯¾ç­–ï¼‰
    y = y[:int(len(y) * 0.9)]
    # MFCCã¨ãã®ãƒ‡ãƒ«ã‚¿
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    delta_mfccs = librosa.feature.delta(mfccs)

    # éŸ³éŸ¿çš„ç‰¹å¾´é‡
    zcr = librosa.feature.zero_crossing_rate(y)
    rms = librosa.feature.rms(y=y)
    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)
    flatness = librosa.feature.spectral_flatness(y=y)
    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)

    # ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ•ãƒ©ãƒƒã‚¯ã‚¹: å‘¨æ³¢æ•°æˆåˆ†ã®æ™‚é–“å¤‰åŒ–
    S = np.abs(librosa.stft(y))
    spectral_flux = np.sqrt(np.mean(np.diff(S, axis=1)**2))

    # ã‚¯ãƒ­ãƒç‰¹å¾´é‡: é«˜ã•æƒ…å ±ã‚’åœ§ç¸®ã—ã¦12æ¬¡å…ƒã§è¡¨ç¾
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)

    # é«˜åŸŸã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆhihatå¯¾ç­–ï¼‰: 4000Hzä»¥ä¸Šã®å¸¯åŸŸã®ãƒ‘ãƒ¯ãƒ¼æ¯”
    freqs = librosa.fft_frequencies(sr=sr)
    high_freq_mask = freqs >= 4000
    high_energy = np.mean(S[high_freq_mask, :])
    total_energy = np.mean(S)
    high_energy_ratio = high_energy / (total_energy + 1e-6)  # 0å‰²é˜²æ­¢

    # ä½åŸŸã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆkickè£œå¼·ç”¨ï¼‰: 0ã€œ250Hzå¸¯åŸŸ
    low_freq_mask = freqs <= 250
    low_energy = np.mean(S[low_freq_mask, :])
    low_energy_ratio = low_energy / (total_energy + 1e-6)

    # å¹³å‡ãƒ»æ¨™æº–åå·®ç­‰ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    features = np.concatenate([
        np.mean(mfccs, axis=1),
        np.std(mfccs, axis=1),
        np.mean(delta_mfccs, axis=1),
        np.std(delta_mfccs, axis=1),
        np.mean(zcr, axis=1),
        np.std(zcr, axis=1),
        np.mean(rms, axis=1),
        np.std(rms, axis=1),
        np.mean(centroid, axis=1),
        np.std(centroid, axis=1),
        np.mean(bandwidth, axis=1),
        np.std(bandwidth, axis=1),
        np.mean(flatness, axis=1),
        np.std(flatness, axis=1),
        np.mean(rolloff, axis=1),
        np.std(rolloff, axis=1),
        np.mean(contrast, axis=1),
        np.std(contrast, axis=1),
        np.array([high_energy_ratio]).flatten(),
        np.array([spectral_flux]).flatten(),
        np.array([low_energy_ratio]).flatten(),  # â˜… è¿½åŠ 
        np.mean(chroma, axis=1),
        np.std(chroma, axis=1)
    ])
    return features

# æŒ‡å®šã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°
def extract_features(file_path):
    print(f"ğŸ“‚ ç‰¹å¾´æŠ½å‡ºä¸­: {file_path}")
    y, sr = librosa.load(file_path, sr=16000)
    return extract_features_from_y(y, sr)

X, y = [], []

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®å„ã‚«ãƒ†ã‚´ãƒªã«ã¤ã„ã¦ã€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
for idx, label in enumerate(CATEGORIES):
    label_dir = os.path.join(DATASET_DIR, label)
    if not os.path.isdir(label_dir):
        print(f"âš ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {label_dir}")
        continue
    for fname in os.listdir(label_dir):
        if fname.endswith(".wav"):
            path = os.path.join(label_dir, fname)
            print(f"ğŸ“‚ ç‰¹å¾´æŠ½å‡ºä¸­: {path}")
            if not os.path.isfile(path):
                continue
            try:
                y_audio, sr = librosa.load(path, sr=16000)
                rms_max = np.max(librosa.feature.rms(y=y_audio))
                # RMSï¼ˆéŸ³é‡ï¼‰ãŒå°ã•ã„å ´åˆã¯ç„¡éŸ³ã¨åˆ¤å®šã—ã€noiseã¨ã—ã¦æ‰±ã†
                if label == "kick" and rms_max < 0.007:
                    print("ğŸ”‡ Kickç„¡éŸ³ã¨åˆ¤æ–­ â†’ noiseç‰¹å¾´é‡")
                    features = np.zeros(105)
                    X.append(features)
                    y.append(idx)
                    continue
                elif rms_max < 0.01:
                    print("ğŸ”‡ ç„¡éŸ³ã¨åˆ¤æ–­ â†’ noiseç‰¹å¾´é‡")
                    features = np.zeros(105)
                    X.append(features)
                    y.append(idx)
                    continue  # æ‹¡å¼µã¯ã‚¹ã‚­ãƒƒãƒ—
                features = extract_features_from_y(y_audio, sr)
                X.append(features)
                y.append(idx)
                # ãƒ”ãƒƒãƒã‚·ãƒ•ãƒˆã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆÂ±2éŸ³ï¼‰
                for n_steps in [-2, 2]:
                    try:
                        y_shifted = librosa.effects.pitch_shift(y_audio, sr=sr, n_steps=n_steps)
                        features_shifted = extract_features_from_y(y_shifted, sr)
                        X.append(features_shifted)
                        y.append(idx)
                    except Exception as e:
                        print(f"âš ï¸ pitch shift ã‚¨ãƒ©ãƒ¼ï¼ˆ{path}, {n_steps}ï¼‰: {e}")
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼ {path}: {e}")

X = np.array(X)
y = np.array(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# ã‚·ãƒ³ãƒ—ãƒ«ãª3å±¤ã®å…¨çµåˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(X.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(len(CATEGORIES), activation='softmax')
])

# ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–æ‰‹æ³•ã¨æå¤±é–¢æ•°ã€è©•ä¾¡æŒ‡æ¨™ã‚’è¨­å®š
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æå¤±ãŒæ”¹å–„ã—ãªããªã£ãŸå ´åˆã«æ—©æœŸçµ‚äº†ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¨­å®š
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®Ÿè¡Œï¼ˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰
model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stop])

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_acc:.4f}")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
os.makedirs("./model", exist_ok=True)
model.save("./model/micrie_model.keras")
print("âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: ./model/micrie_model.keras")

# æŒ‡å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ãƒ³ãƒã«åŸºã¥ã„ã¦8åˆ†å‰²ã—ã€ãã‚Œãã‚Œã«å¯¾ã—ã¦æ¨è«–ã‚’å®Ÿè¡Œ
def predict(file_path, model, tempo):
    y_full, sr = librosa.load(file_path, sr=16000)
    total_duration = 60.0 / tempo * 4
    chunk_duration = total_duration / 8

    onset_env = librosa.onset.onset_strength(y=y_full, sr=sr)
    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr, backtrack=True)
    onset_times = librosa.frames_to_time(onset_frames, sr=sr)

    features_list = []
    adjusted_starts = []
    results = []
    for i in range(8):
        chunk_start = i * chunk_duration
        search_start = chunk_start - 0.5 * chunk_duration
        # ã‚¹ãƒŠãƒƒãƒ—å¯¾è±¡ã¯ãƒãƒ£ãƒ³ã‚¯å…ˆé ­ã‚ˆã‚Šå‰ï¼ˆÂ±0.5ãƒãƒ£ãƒ³ã‚¯å†…ï¼‰ã®onsetã®ã¿ã«é™å®š
        candidate_onsets = [onset for onset in onset_times if search_start <= onset < chunk_start]

        # ãƒãƒ£ãƒ³ã‚¯ã®é–‹å§‹ä½ç½®ã‚’ã€è¿‘å‚ã®onsetï¼ˆç™ºéŸ³é–‹å§‹ï¼‰ã«ã‚¹ãƒŠãƒƒãƒ—ã—ã¦èª¿æ•´
        if candidate_onsets:
            onset = max(candidate_onsets)
            shift = onset - chunk_start
            adjusted_start = chunk_start + shift
        else:
            adjusted_start = chunk_start

        adjusted_end = adjusted_start + chunk_duration
        y_chunk = y_full[int(sr * adjusted_start) : int(sr * adjusted_end)]

        rms = librosa.feature.rms(y=y_chunk)
        # RMSãŒå°ã•ã„å ´åˆã¯ç„¡éŸ³ã¨åˆ¤å®šã—ã€noiseã¨ã—ã¦å‡¦ç†
        if np.max(rms) < 0.01:
            results.append({
                "label": "noise",
                "start": round(chunk_start, 2),
                "end": round(chunk_start + chunk_duration, 2),
                "adjustedStart": round(adjusted_start, 4),
                "scores": [0, 0, 0, 1]
            })
            features = np.zeros(105)
            features_list.append(features)
            adjusted_starts.append(adjusted_start)
            continue

        features = extract_features_from_y(y_chunk, sr)
        features_list.append(features)
        adjusted_starts.append(adjusted_start)

    # æœ‰åŠ¹ãªãƒãƒ£ãƒ³ã‚¯ãŒå­˜åœ¨ã™ã‚‹å ´åˆã«æ¨è«–å‡¦ç†ã‚’è¡Œã†
    if features_list:
        features_array = np.array(features_list)
        predictions = model.predict(features_array)
        predicted_labels = np.argmax(predictions, axis=1)

        label_names = CATEGORIES
        j = 0
        for i in range(8):
            chunk_start = i * chunk_duration
            chunk_end = chunk_start + chunk_duration
            # ã™ã§ã«noiseã¨ã—ã¦çµæœãŒå…¥ã£ã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if any(r["start"] == round(chunk_start, 2) for r in results):
                continue
            results.append({
                "label": label_names[predicted_labels[j]],
                "start": round(chunk_start, 2),
                "end": round(chunk_end, 2),
                "adjustedStart": round(adjusted_starts[j], 4),  # ã‚¹ãƒŠãƒƒãƒ—å¾Œã®è§£æé–‹å§‹ä½ç½®ã‚’è¿½åŠ 
                "scores": [round(score, 6) for score in predictions[j].tolist()]
            })
            j += 1
    return results