# -----------------------------------------------
# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘å–ã‚Šã€
# ãƒ†ãƒ³ãƒã«åŸºã¥ã„ã¦éŸ³å£°ã‚’8åˆ†å‰²ã—ã€å„ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã¦åˆ†é¡
# æ¨è«–çµæœã¯JSONã¨ã—ã¦è¿”å´ã—ã€CSVã«ãƒ­ã‚°ã¨ã—ã¦ä¿å­˜
# Flaskã®Blueprintã‚’ä½¿ç”¨ã—ã¦ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æä¾›
# -----------------------------------------------
from flask import Blueprint, request, jsonify
import numpy as np
import librosa
import os
import tempfile
import ffmpeg
from keras.models import load_model
import csv
from datetime import datetime


# éŸ³å£°ä¿¡å·ã‹ã‚‰æ§˜ã€…ãªéŸ³éŸ¿ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã¦ã€1æ¬¡å…ƒã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è¿”ã™é–¢æ•°
# æ‹¡å¼µã•ã‚ŒãŸç‰¹å¾´æŠ½å‡ºé–¢æ•°
def extract_features(y, sr):
    print("ğŸ“‚ ç‰¹å¾´æŠ½å‡ºä¸­ï¼ˆæ¨è«–ï¼‰")

    # ç„¡éŸ³åˆ¤å®š: RMSæœ€å¤§å€¤ãŒé–¾å€¤æœªæº€ãªã‚‰noiseã¨ã¿ãªã™
    if np.max(librosa.feature.rms(y=y)) < 0.05:
        print("ğŸ”‡ ç„¡éŸ³ã¨åˆ¤æ–­: noiseç‰¹å¾´é‡ã‚’è¿”ã—ã¾ã™")
        return np.zeros(104) 

    """æ‹¡å¼µã•ã‚ŒãŸç‰¹å¾´æŠ½å‡ºé–¢æ•°: MFCC(13), Î”MFCC, ZCR, RMS, Centroidãªã©"""
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    width = min(5, mfccs.shape[1] // 2 * 2 + 1)
    delta_mfccs = librosa.feature.delta(mfccs, width=width)
    zcr = librosa.feature.zero_crossing_rate(y)
    rms = librosa.feature.rms(y=y)
    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)
    flatness = librosa.feature.spectral_flatness(y=y)
    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)

    freqs = librosa.fft_frequencies(sr=sr)
    S = np.abs(librosa.stft(y))
    high_freq_mask = freqs >= 4000
    high_energy = np.mean(S[high_freq_mask, :])
    total_energy = np.mean(S)
    high_energy_ratio = high_energy / (total_energy + 1e-6)

    spectral_flux = np.sqrt(np.mean(np.diff(S, axis=1)**2))
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)

    feature_vector = np.concatenate([
      np.mean(mfccs, axis=1),
      np.std(mfccs, axis=1),
      np.mean(delta_mfccs, axis=1),
      np.std(delta_mfccs, axis=1),
      np.mean(zcr, axis=1),
      np.std(zcr, axis=1),
      np.mean(rms, axis=1),
      np.std(rms, axis=1),
      np.mean(centroid, axis=1),
      np.std(centroid, axis=1),
      np.mean(bandwidth, axis=1),
      np.std(bandwidth, axis=1),
      np.mean(flatness, axis=1),
      np.std(flatness, axis=1),
      np.mean(rolloff, axis=1),
      np.std(rolloff, axis=1),
      np.mean(contrast, axis=1),
      np.std(contrast, axis=1),
      np.array([spectral_flux]).flatten(),
      np.array([high_energy_ratio]), 
      np.mean(chroma, axis=1),
      np.std(chroma, axis=1)
    ])
    if feature_vector.shape[0] != 104:
        print(f"âš ï¸ ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒãŒä¸æ­£ã§ã™: {feature_vector.shape}")
    return feature_vector

# æ¨è«–çµæœï¼ˆå„ãƒãƒ£ãƒ³ã‚¯ã®ãƒ©ãƒ™ãƒ«ã¨ã‚¹ã‚³ã‚¢ï¼‰ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ã™ã‚‹é–¢æ•°
CSV_LOG_PATH = "./prediction_log.csv"

def log_to_csv(segments):
    """ãƒãƒ£ãƒ³ã‚¯æ¯ã®ã‚¹ã‚³ã‚¢ã¨äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã‚’CSVã«è¿½è¨˜"""
    file_exists = os.path.isfile(CSV_LOG_PATH)

    with open(CSV_LOG_PATH, mode='a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["timestamp", "chunk", "start", "end", "kick", "snare", "hihat", "noise", "label"])

        timestamp = datetime.now().isoformat()
        for i, seg in enumerate(segments):
            writer.writerow([
                timestamp,
                f"chunk_{i}",
                seg["start"],
                seg["end"],
                *seg["scores"],
                seg["label"]
            ])


predict_bp = Blueprint("predict", __name__)

# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨åˆæœŸè¨­å®š
model_path = os.path.join(os.path.dirname(__file__), "model", "micrie_model.keras")
print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {model_path}")
model = load_model(model_path)
print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
labels = ["kick", "snare", "hihat", "noise"]

# /predict ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼šéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘å–ã‚Šã€ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ç‰¹å¾´é‡æŠ½å‡ºã¨æ¨è«–ã‚’è¡Œã„ã€çµæœã‚’è¿”ã™
@predict_bp.route("/predict", methods=["POST"])
def predict():
    file = request.files["file"]
    tempo = float(request.form.get("tempo", 120))

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚çš„ã«ä¿å­˜ã—ã€webmå½¢å¼ã‹ã‚‰wavå½¢å¼ã«å¤‰æ›
    with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as tmp_webm:
        file.save(tmp_webm.name)
        webm_path = tmp_webm.name

    wav_path = webm_path.replace(".webm", ".wav")

    try:
        # webm â†’ wav ã«å¤‰æ›
        ffmpeg.input(webm_path).output(wav_path).run(quiet=True, overwrite_output=True)

        # å…¨ä½“ã®éŸ³å£°ã‚’èª­ã¿è¾¼ã¿
        y_full, sr = librosa.load(wav_path, sr=16000)

        # éŸ³å£°å…¨ä½“ã‹ã‚‰ã‚ªãƒ³ã‚»ãƒƒãƒˆï¼ˆç™ºéŸ³ã®å§‹ã¾ã‚Šï¼‰ã‚’æ¤œå‡ºã—ã¦æ™‚é–“æƒ…å ±ã‚’å–å¾—
        onset_env = librosa.onset.onset_strength(y=y_full, sr=sr)
        onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr, backtrack=True)
        onset_times = librosa.frames_to_time(onset_frames, sr=sr)

        if len(onset_times) < 9:
            duration = librosa.get_duration(y=y_full, sr=sr)
            onset_times = list(onset_times) + [duration] * (9 - len(onset_times))
        onset_times = onset_times[:9]

        results = []

        # ãƒ†ãƒ³ãƒæƒ…å ±ã«åŸºã¥ã„ã¦1å°ç¯€ã‚’8ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹è¨­å®š
        beat_duration = 60.0 / tempo
        total_duration = beat_duration * 4
        chunk_duration = total_duration / 8

        # å„ãƒãƒ£ãƒ³ã‚¯ã«ã¤ã„ã¦ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã€æ¨è«–ã‚’è¡Œã†
        for i in range(8):
            nominal_start = i * chunk_duration
            nominal_end = nominal_start + chunk_duration
            search_start = max(0, nominal_start - chunk_duration / 2)
            search_end = nominal_start

            # ã‚¹ãƒŠãƒƒãƒ—å¯¾è±¡ã¨ãªã‚‹ onset ã‚’æ¢ç´¢ï¼ˆå‰ã®ã¿ï¼‰
            candidate_onsets = [t for t in onset_times if search_start <= t < search_end]
            if candidate_onsets:
                onset = max(candidate_onsets)
                shift = onset - nominal_start
            else:
                shift = 0.0

            adjusted_start = nominal_start + shift
            adjusted_end = adjusted_start + chunk_duration
            y_chunk = y_full[int(sr * adjusted_start):int(sr * adjusted_end)]
            y_chunk = y_chunk[:int(len(y_chunk) * 0.9)]  # ãƒãƒ£ãƒ³ã‚¯ã®æœ«å°¾10%ã‚’ã‚«ãƒƒãƒˆï¼ˆã‚ã‚Šè¾¼ã¿é˜²æ­¢ï¼‰

            # ç„¡éŸ³ã‚„éŸ³å£°ãŒçŸ­ã™ãã‚‹å ´åˆã¯ noise ã¨ã—ã¦æ‰±ã†
            if y_chunk.shape[0] == 0:
                results.append({"label": "noise", "start": round(nominal_start, 2), "end": round(nominal_end, 2), "adjustedStart": round(adjusted_start, 4), "scores": [0,0,0,1]})
                continue

            feature_vector = extract_features(y_chunk, sr)
            feature_vector = feature_vector.reshape(1, -1)

            try:
                print("ğŸ“ ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ« shape:", feature_vector.shape)
                print("ğŸ” ãƒ¢ãƒ‡ãƒ« expected input shape:", model.input_shape)
                pred = model.predict(feature_vector)
                predicted_index = np.argmax(pred[0])
            except Exception as e:
                print("âŒ æ¨è«–ã‚¨ãƒ©ãƒ¼:", str(e))
                raise

            label = labels[predicted_index]

            print(f"ğŸ¯ ãƒãƒ£ãƒ³ã‚¯ {i}: {round(adjusted_start,2)}s ~ {round(adjusted_end,2)}s")
            print("    ğŸ”¢ äºˆæ¸¬ã‚¹ã‚³ã‚¢:", pred[0])
            print("    ğŸ·ï¸ äºˆæ¸¬ãƒ©ãƒ™ãƒ«:", label)

            results.append({
                "label": label,
                "start": round(nominal_start, 2),
                "end": round(nominal_end, 2),
                "adjustedStart": round(adjusted_start, 4),
                "scores": [round(score, 6) for score in pred[0].tolist()]
            })

        # æ¨è«–çµæœã‚’CSVãƒ­ã‚°ã«ä¿å­˜
        log_to_csv(results)
        return jsonify({"segments": results})
    
    except Exception as e:
        print("ğŸ”¥ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ:", str(e))
        return jsonify({"error": str(e)}), 500

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¦å¾Œå‡¦ç†
    finally:
        for path in [webm_path, wav_path]:
            if os.path.exists(path):
                os.remove(path)


__all__ = ["predict_bp"]